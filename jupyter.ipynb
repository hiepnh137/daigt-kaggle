{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('train_essays_3.0.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train essays dataframe has shape: (32722, 4)\n",
      "fold  generated\n",
      "0     0             1000\n",
      "      1             1000\n",
      "1     0            26371\n",
      "      1             4351\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm \n",
    "\n",
    "\n",
    "train_df = pd.read_csv('train_essays_3.0.csv')\n",
    "print(f\"Train essays dataframe has shape: {train_df.shape}\")\n",
    "\n",
    "'''\n",
    "    Stratified K Fold\n",
    "'''\n",
    "\n",
    "real_indice = train_df[train_df['generated']==0].index\n",
    "fake_indice = train_df[train_df['generated']==1].index\n",
    "real_choice_valid = np.random.choice(real_indice, 1000, replace=False)\n",
    "fake_choice_valid = np.random.choice(fake_indice, 1000, replace=False)\n",
    "train_df['fold'] = 1\n",
    "train_df.loc[real_choice_valid, 'fold'] = 0\n",
    "train_df.loc[fake_choice_valid, 'fold'] = 0\n",
    "# skf = StratifiedKFold(n_splits=5)\n",
    "\n",
    "# X = train_df.loc[:, train_df.columns != \"generated\"]\n",
    "# y = train_df.loc[:, train_df.columns == \"generated\"]\n",
    "\n",
    "\n",
    "\n",
    "# for i, (train_index, valid_index) in enumerate(skf.split(X, y)):\n",
    "#     train_df.loc[valid_index, \"fold\"] = i\n",
    "    \n",
    "print(train_df.groupby(\"fold\")[\"generated\"].value_counts())\n",
    "train_df.head()\n",
    "\n",
    "oof_df = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "folds = train_df\n",
    "fold = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch\n",
    "def prepare_input(cfg, text, tokenizer):\n",
    "    \"\"\"\n",
    "    This function tokenizes the input text with the configured padding and truncation. Then,\n",
    "    returns the input dictionary, which contains the following keys: \"input_ids\",\n",
    "    \"token_type_ids\" and \"attention_mask\". Each value is a torch.tensor.\n",
    "    :param cfg: configuration class with a TOKENIZER attribute.\n",
    "    :param text: a numpy array where each value is a text as string.\n",
    "    :return inputs: python dictionary where values are torch tensors.\n",
    "    \"\"\"\n",
    "    inputs = tokenizer.encode_plus(\n",
    "        text, \n",
    "        return_tensors=None, \n",
    "        add_special_tokens=True, \n",
    "        max_length=cfg.MAX_LEN,\n",
    "        padding='max_length', # TODO: check padding to max sequence in batch\n",
    "        truncation=True\n",
    "    )\n",
    "    for k, v in inputs.items():\n",
    "        inputs[k] = torch.tensor(v, dtype=torch.long) # TODO: check dtypes\n",
    "    return inputs\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, cfg, df, tokenizer):\n",
    "        self.cfg = cfg\n",
    "        self.texts = df['text'].values\n",
    "        self.labels = df['generated'].values\n",
    "        self.tokenizer = tokenizer\n",
    "        self.text_ids = df['id'].values\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        output = {}\n",
    "        output[\"inputs\"] = prepare_input(self.cfg, self.texts[item], self.tokenizer)\n",
    "        output[\"labels\"] = torch.tensor(self.labels[item], dtype=torch.float) # TODO: check dtypes\n",
    "        output[\"ids\"] = self.text_ids[item]\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Couldn't instantiate the backend tokenizer from one of: \n(1) a `tokenizers` library serialization file, \n(2) a slow tokenizer instance to convert or \n(3) an equivalent slow tokenizer class to instantiate and convert. \nYou need to have sentencepiece installed to convert a slow tokenizer to a fast one.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 48\u001b[0m\n\u001b[1;32m     39\u001b[0m dataset \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mconcat([label0_dataset,\n\u001b[1;32m     40\u001b[0m                 label1_dataset,\n\u001b[1;32m     41\u001b[0m                 label1_dataset,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     45\u001b[0m                 ], \n\u001b[1;32m     46\u001b[0m                 ignore_index\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m     47\u001b[0m train_folds \u001b[39m=\u001b[39m dataset\n\u001b[0;32m---> 48\u001b[0m tokenizer \u001b[39m=\u001b[39m AutoTokenizer\u001b[39m.\u001b[39;49mfrom_pretrained(\u001b[39m\"\u001b[39;49m\u001b[39mmicrosoft/deberta-v3-base\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m     49\u001b[0m train_dataset \u001b[39m=\u001b[39m CustomDataset(config, train_folds, tokenizer)\n\u001b[1;32m     50\u001b[0m valid_dataset \u001b[39m=\u001b[39m CustomDataset(config, valid_folds, tokenizer)\n",
      "File \u001b[0;32m~/Documents/code/kaggle/daigt-kaggle/.env/lib/python3.8/site-packages/transformers/models/auto/tokenization_auto.py:786\u001b[0m, in \u001b[0;36mAutoTokenizer.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    784\u001b[0m tokenizer_class_py, tokenizer_class_fast \u001b[39m=\u001b[39m TOKENIZER_MAPPING[\u001b[39mtype\u001b[39m(config)]\n\u001b[1;32m    785\u001b[0m \u001b[39mif\u001b[39;00m tokenizer_class_fast \u001b[39mand\u001b[39;00m (use_fast \u001b[39mor\u001b[39;00m tokenizer_class_py \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m):\n\u001b[0;32m--> 786\u001b[0m     \u001b[39mreturn\u001b[39;00m tokenizer_class_fast\u001b[39m.\u001b[39;49mfrom_pretrained(pretrained_model_name_or_path, \u001b[39m*\u001b[39;49minputs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    787\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    788\u001b[0m     \u001b[39mif\u001b[39;00m tokenizer_class_py \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/Documents/code/kaggle/daigt-kaggle/.env/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2024\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, cache_dir, force_download, local_files_only, token, revision, *init_inputs, **kwargs)\u001b[0m\n\u001b[1;32m   2021\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   2022\u001b[0m         logger\u001b[39m.\u001b[39minfo(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mloading file \u001b[39m\u001b[39m{\u001b[39;00mfile_path\u001b[39m}\u001b[39;00m\u001b[39m from cache at \u001b[39m\u001b[39m{\u001b[39;00mresolved_vocab_files[file_id]\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m-> 2024\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mcls\u001b[39;49m\u001b[39m.\u001b[39;49m_from_pretrained(\n\u001b[1;32m   2025\u001b[0m     resolved_vocab_files,\n\u001b[1;32m   2026\u001b[0m     pretrained_model_name_or_path,\n\u001b[1;32m   2027\u001b[0m     init_configuration,\n\u001b[1;32m   2028\u001b[0m     \u001b[39m*\u001b[39;49minit_inputs,\n\u001b[1;32m   2029\u001b[0m     token\u001b[39m=\u001b[39;49mtoken,\n\u001b[1;32m   2030\u001b[0m     cache_dir\u001b[39m=\u001b[39;49mcache_dir,\n\u001b[1;32m   2031\u001b[0m     local_files_only\u001b[39m=\u001b[39;49mlocal_files_only,\n\u001b[1;32m   2032\u001b[0m     _commit_hash\u001b[39m=\u001b[39;49mcommit_hash,\n\u001b[1;32m   2033\u001b[0m     _is_local\u001b[39m=\u001b[39;49mis_local,\n\u001b[1;32m   2034\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m   2035\u001b[0m )\n",
      "File \u001b[0;32m~/Documents/code/kaggle/daigt-kaggle/.env/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2256\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase._from_pretrained\u001b[0;34m(cls, resolved_vocab_files, pretrained_model_name_or_path, init_configuration, token, cache_dir, local_files_only, _commit_hash, _is_local, *init_inputs, **kwargs)\u001b[0m\n\u001b[1;32m   2254\u001b[0m \u001b[39m# Instantiate the tokenizer.\u001b[39;00m\n\u001b[1;32m   2255\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 2256\u001b[0m     tokenizer \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39;49m(\u001b[39m*\u001b[39;49minit_inputs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49minit_kwargs)\n\u001b[1;32m   2257\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mOSError\u001b[39;00m:\n\u001b[1;32m   2258\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mOSError\u001b[39;00m(\n\u001b[1;32m   2259\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mUnable to load vocabulary from file. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   2260\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mPlease check that the provided vocabulary is accessible and not corrupted.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   2261\u001b[0m     )\n",
      "File \u001b[0;32m~/Documents/code/kaggle/daigt-kaggle/.env/lib/python3.8/site-packages/transformers/models/deberta_v2/tokenization_deberta_v2_fast.py:133\u001b[0m, in \u001b[0;36mDebertaV2TokenizerFast.__init__\u001b[0;34m(self, vocab_file, tokenizer_file, do_lower_case, split_by_punct, bos_token, eos_token, unk_token, sep_token, pad_token, cls_token, mask_token, **kwargs)\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\n\u001b[1;32m    119\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    120\u001b[0m     vocab_file\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    131\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[1;32m    132\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 133\u001b[0m     \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__init__\u001b[39;49m(\n\u001b[1;32m    134\u001b[0m         vocab_file,\n\u001b[1;32m    135\u001b[0m         tokenizer_file\u001b[39m=\u001b[39;49mtokenizer_file,\n\u001b[1;32m    136\u001b[0m         do_lower_case\u001b[39m=\u001b[39;49mdo_lower_case,\n\u001b[1;32m    137\u001b[0m         bos_token\u001b[39m=\u001b[39;49mbos_token,\n\u001b[1;32m    138\u001b[0m         eos_token\u001b[39m=\u001b[39;49meos_token,\n\u001b[1;32m    139\u001b[0m         unk_token\u001b[39m=\u001b[39;49munk_token,\n\u001b[1;32m    140\u001b[0m         sep_token\u001b[39m=\u001b[39;49msep_token,\n\u001b[1;32m    141\u001b[0m         pad_token\u001b[39m=\u001b[39;49mpad_token,\n\u001b[1;32m    142\u001b[0m         cls_token\u001b[39m=\u001b[39;49mcls_token,\n\u001b[1;32m    143\u001b[0m         mask_token\u001b[39m=\u001b[39;49mmask_token,\n\u001b[1;32m    144\u001b[0m         split_by_punct\u001b[39m=\u001b[39;49msplit_by_punct,\n\u001b[1;32m    145\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m    146\u001b[0m     )\n\u001b[1;32m    148\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdo_lower_case \u001b[39m=\u001b[39m do_lower_case\n\u001b[1;32m    149\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msplit_by_punct \u001b[39m=\u001b[39m split_by_punct\n",
      "File \u001b[0;32m~/Documents/code/kaggle/daigt-kaggle/.env/lib/python3.8/site-packages/transformers/tokenization_utils_fast.py:120\u001b[0m, in \u001b[0;36mPreTrainedTokenizerFast.__init__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    118\u001b[0m     fast_tokenizer \u001b[39m=\u001b[39m convert_slow_tokenizer(slow_tokenizer)\n\u001b[1;32m    119\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 120\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    121\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mCouldn\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt instantiate the backend tokenizer from one of: \u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    122\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m(1) a `tokenizers` library serialization file, \u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    123\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m(2) a slow tokenizer instance to convert or \u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    124\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m(3) an equivalent slow tokenizer class to instantiate and convert. \u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    125\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mYou need to have sentencepiece installed to convert a slow tokenizer to a fast one.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    126\u001b[0m     )\n\u001b[1;32m    128\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_tokenizer \u001b[39m=\u001b[39m fast_tokenizer\n\u001b[1;32m    130\u001b[0m \u001b[39mif\u001b[39;00m slow_tokenizer \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[0;31mValueError\u001b[0m: Couldn't instantiate the backend tokenizer from one of: \n(1) a `tokenizers` library serialization file, \n(2) a slow tokenizer instance to convert or \n(3) an equivalent slow tokenizer class to instantiate and convert. \nYou need to have sentencepiece installed to convert a slow tokenizer to a fast one."
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "class config:\n",
    "    APEX = True # Automatic Precision Enabled\n",
    "    BATCH_SCHEDULER = True\n",
    "    BATCH_SIZE_TRAIN = 32\n",
    "    BATCH_SIZE_VALID = 16\n",
    "    BETAS = (0.9, 0.999)\n",
    "    DEBUG = False\n",
    "    DECODER_LR = 2e-5\n",
    "    ENCODER_LR = 2e-5\n",
    "    EPOCHS = 5\n",
    "    EPS = 1e-6\n",
    "    FOLDS = 2\n",
    "    GRADIENT_ACCUMULATION_STEPS = 1\n",
    "    GRADIENT_CHECKPOINTING = True\n",
    "    MAX_GRAD_NORM=1000\n",
    "    MAX_LEN = 512\n",
    "    MIN_LR = 1e-6\n",
    "    MODEL = \"microsoft/deberta-v3-base\"\n",
    "    NUM_CYCLES = 0.5\n",
    "    NUM_WARMUP_STEPS = 0\n",
    "    PRINT_FREQ = 20\n",
    "    SCHEDULER = 'cosine' # ['linear', 'cosine']\n",
    "    SEED = 27\n",
    "    TRAIN = True\n",
    "    TRAIN_FOLDS = [0, 1, 2, 3]\n",
    "    WANDB = True\n",
    "    WEIGHT_DECAY = 0.01\n",
    "\n",
    "\n",
    "train_folds = folds[folds['fold'] != fold].reset_index(drop=True)\n",
    "valid_folds = folds[folds['fold'] == fold].reset_index(drop=True)\n",
    "\n",
    "valid_labels = valid_folds['generated'].values\n",
    "\n",
    "# ======== DATASETS ==========\n",
    "label0_dataset = train_folds[train_folds['generated']==0]\n",
    "label1_dataset = train_folds[train_folds['generated']==1]\n",
    "dataset = pd.concat([label0_dataset,\n",
    "                label1_dataset,\n",
    "                label1_dataset,\n",
    "                label1_dataset,\n",
    "                label1_dataset,\n",
    "                label1_dataset,\n",
    "                ], \n",
    "                ignore_index=True)\n",
    "train_folds = dataset\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/deberta-v3-base\")\n",
    "train_dataset = CustomDataset(config, train_folds, tokenizer)\n",
    "valid_dataset = CustomDataset(config, valid_folds, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "choice = np.random.choice(real_indice, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3495     0\n",
       "7417     0\n",
       "9294     0\n",
       "18564    0\n",
       "22473    0\n",
       "        ..\n",
       "20156    0\n",
       "23124    0\n",
       "681      0\n",
       "3117     0\n",
       "25264    0\n",
       "Name: generated, Length: 1000, dtype: int64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[choice, 'generated']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
