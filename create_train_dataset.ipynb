{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "PATH = 'data/'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>prompt_id</th>\n",
       "      <th>text</th>\n",
       "      <th>generated</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0059830c</td>\n",
       "      <td>0</td>\n",
       "      <td>Cars. Cars have been around since they became ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>005db917</td>\n",
       "      <td>0</td>\n",
       "      <td>Transportation is a large necessity in most co...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>008f63e3</td>\n",
       "      <td>0</td>\n",
       "      <td>\"America's love affair with it's vehicles seem...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00940276</td>\n",
       "      <td>0</td>\n",
       "      <td>How often do you ride in a car? Do you drive a...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>00c39458</td>\n",
       "      <td>0</td>\n",
       "      <td>Cars are a wonderful thing. They are perhaps o...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1373</th>\n",
       "      <td>fe6ff9a5</td>\n",
       "      <td>1</td>\n",
       "      <td>There has been a fuss about the Elector Colleg...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1374</th>\n",
       "      <td>ff669174</td>\n",
       "      <td>0</td>\n",
       "      <td>Limiting car usage has many advantages. Such a...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1375</th>\n",
       "      <td>ffa247e0</td>\n",
       "      <td>0</td>\n",
       "      <td>There's a new trend that has been developing f...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1376</th>\n",
       "      <td>ffc237e9</td>\n",
       "      <td>0</td>\n",
       "      <td>As we all know cars are a big part of our soci...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1377</th>\n",
       "      <td>ffe1ca0d</td>\n",
       "      <td>0</td>\n",
       "      <td>Cars have been around since the 1800's and hav...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1378 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            id  prompt_id                                               text  \\\n",
       "0     0059830c          0  Cars. Cars have been around since they became ...   \n",
       "1     005db917          0  Transportation is a large necessity in most co...   \n",
       "2     008f63e3          0  \"America's love affair with it's vehicles seem...   \n",
       "3     00940276          0  How often do you ride in a car? Do you drive a...   \n",
       "4     00c39458          0  Cars are a wonderful thing. They are perhaps o...   \n",
       "...        ...        ...                                                ...   \n",
       "1373  fe6ff9a5          1  There has been a fuss about the Elector Colleg...   \n",
       "1374  ff669174          0  Limiting car usage has many advantages. Such a...   \n",
       "1375  ffa247e0          0  There's a new trend that has been developing f...   \n",
       "1376  ffc237e9          0  As we all know cars are a big part of our soci...   \n",
       "1377  ffe1ca0d          0  Cars have been around since the 1800's and hav...   \n",
       "\n",
       "      generated  \n",
       "0             0  \n",
       "1             0  \n",
       "2             0  \n",
       "3             0  \n",
       "4             0  \n",
       "...         ...  \n",
       "1373          0  \n",
       "1374          0  \n",
       "1375          0  \n",
       "1376          0  \n",
       "1377          0  \n",
       "\n",
       "[1378 rows x 4 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_essays = pd.read_csv(PATH + 'llm-detect-ai-generated-text/train_essays.csv')\n",
    "train_essays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "Radek_data_gpt_3_5 = pd.read_csv(PATH + 'llm-generated-essays/ai_generated_train_essays.csv')\n",
    "Radek_data_gpt_4   = pd.read_csv(PATH + 'llm-generated-essays/ai_generated_train_essays_gpt-4.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25996, 2)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PERSUADE_corpus = pd.read_csv(PATH + \"persaude-corpus-2/persuade_2.0_human_scores_demo_id_github.csv\")\n",
    "PERSUADE_corpus = PERSUADE_corpus[\"full_text\"].to_frame()\n",
    "PERSUADE_corpus[\"generated\"] = 0 # human data\n",
    "PERSUADE_corpus = PERSUADE_corpus.rename(columns = {'full_text':'text'})\n",
    "PERSUADE_corpus.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "llama_70b_dataset = pd.read_csv(PATH + \"daigt-data-llama-70b-and-falcon180b/llama_70b_v1.csv\")\n",
    "llama_70b_dataset = llama_70b_dataset[\"generated_text\"].to_frame()\n",
    "llama_70b_dataset[\"generated\"] = 1 # AI-LLM data\n",
    "llama_70b_dataset = llama_70b_dataset.rename(columns = {'generated_text':'text'})\n",
    "\n",
    "falcon_180b_dataset = pd.read_csv(PATH + \"daigt-data-llama-70b-and-falcon180b/falcon_180b_v1.csv\")\n",
    "falcon_180b_dataset = falcon_180b_dataset[\"generated_text\"].to_frame()\n",
    "falcon_180b_dataset[\"generated\"] = 1 # AI-LLM data\n",
    "falcon_180b_dataset = falcon_180b_dataset.rename(columns = {'generated_text':'text'})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2421, 2)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "daigt_external_dataset = pd.read_csv(PATH + \"daigt-external-dataset/daigt_external_dataset.csv\")\n",
    "daigt_external_dataset = daigt_external_dataset['text'].to_frame()\n",
    "daigt_external_dataset[\"generated\"] = 1\n",
    "daigt_external_dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import random\n",
    "import numpy as np\n",
    "from tqdm import tqdm \n",
    "\n",
    "\n",
    "BRACKET_SYMBOL = ['[', ']', '(', ')', '{', '}']\n",
    "SPECIAL_CHARACTERS = ['.', '+', '*', '?', '^', '$', '(', ')', '[', ']', '{', '}', '|', '\\\\']\n",
    "CHARACTERS = 'abcdefghijklmnopqrstuvwxyz'\n",
    "VOWEL = 'ueoai'\n",
    "CONSONANTS = 'bcdfghjklmnpqrstvwxz'\n",
    "\n",
    "\n",
    "\n",
    "def normalize_text(text):\n",
    "    text = text.replace('-', '')\n",
    "    for symbol in BRACKET_SYMBOL:\n",
    "        text = text.replace(symbol, f' {symbol} ')\n",
    "\n",
    "    text = re.sub(' +', ' ', text)\n",
    "    return text\n",
    "\n",
    "def is_word(word):\n",
    "    for c in SPECIAL_CHARACTERS:\n",
    "        if c in word:\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "class Noise:    \n",
    "    def remove_consonant(self, words, rate=0.2):\n",
    "        for i, word in enumerate(words):\n",
    "            if is_word(word):\n",
    "                c = random.choice(CONSONANTS)\n",
    "                prob = np.random.uniform(0, 1, 1)\n",
    "                if prob[0] < rate:\n",
    "                    words[i] = words[i].replace(c, '')\n",
    "\n",
    "        return words\n",
    "    \n",
    "    def replace_consonant(self, words):\n",
    "        v = random.choice(VOWEL)\n",
    "        c = random.choice(CONSONANTS)\n",
    "        for i, word in enumerate(words):\n",
    "            if is_word(word):\n",
    "                words[i] = words[i].replace(c, v)\n",
    "                words[i] = words[i].replace(c.upper(), v.upper())\n",
    "        return words\n",
    "    \n",
    "    def remove_space(self, words, n=3):\n",
    "        sentence = ' '.join(words)\n",
    "        space_ids = np.random.randint(0, len(words)-2, n)\n",
    "        phrases = []\n",
    "        for i in space_ids:\n",
    "            phrases.append(f'{words[i]} {words[i+1]}')\n",
    "        for phrase in phrases:\n",
    "            sentence = sentence.replace(phrase, phrase.replace(' ', ''))\n",
    "        return sentence.split()\n",
    "    \n",
    "    def insert_vowel(self, words, rate=0.4):\n",
    "        v = random.choice(VOWEL)\n",
    "        unique_words = list(set(words))\n",
    "        chosen_words = np.random.choice(unique_words, int(rate*len(unique_words)))\n",
    "        inserted_words = []\n",
    "        for word in chosen_words:\n",
    "            id = random.randint(0, len(word)+1)\n",
    "            inserted_words.append(word[:id] + v + word[id:])\n",
    "        sentence = ' '.join(words)\n",
    "        for w, r in zip(chosen_words, inserted_words):\n",
    "            sentence = sentence.replace(w, r)\n",
    "        words = sentence.split()\n",
    "#         print(' '.join(words))\n",
    "#         print('inserted_words: ', inserted_words)\n",
    "        return words\n",
    "    \n",
    "    def randomly_lower(self, words, rate=0.3):\n",
    "        special_words = []\n",
    "        for i, word in enumerate(words):\n",
    "            if i == 0:\n",
    "                continue\n",
    "            if word[0] == word[0].upper() and not is_word(words[i-1]):\n",
    "                special_words.append(word)\n",
    "        n = int(rate*len(special_words)+1)\n",
    "        if n > len(special_words):\n",
    "            return words\n",
    "        chosen_words = np.random.choice(special_words, n)\n",
    "        \n",
    "        sentence = ' '.join(words)\n",
    "        for word in chosen_words:\n",
    "            sentence = sentence.replace(word, word.lower())\n",
    "        \n",
    "        return sentence.split()\n",
    "    \n",
    "    def remove_apostrophe(self, words, rate=0.2):\n",
    "        sentence = ' '.join(words)\n",
    "        apost_phrase = re.findall('[^ ]*\\'[^ ]*', sentence)\n",
    "        if int(rate*len(apost_phrase)+1) > len(apost_phrase):\n",
    "            return words\n",
    "        chosen_words = np.random.choice(apost_phrase, int(rate*len(apost_phrase)+1))\n",
    "        for word in chosen_words:\n",
    "            sentence = sentence.replace(word, word.replace(\"'\", \"\"))\n",
    "        return sentence.split()\n",
    "    \n",
    "    def add_noise_to_corpus(self, corpus, rate=0.9):\n",
    "        prob = np.random.uniform(0, 1, len(corpus))\n",
    "        choice = prob > rate\n",
    "        new_corpus = []\n",
    "        for i in tqdm(range(len(corpus))):\n",
    "            if choice[i]:\n",
    "                new_corpus.append(corpus[i])\n",
    "            else:\n",
    "                words = corpus[i].split()\n",
    "                noise_id = np.random.randint(0, 2, 6)\n",
    "                if noise_id[0] == 1:\n",
    "                    words = self.replace_consonant(words)\n",
    "                if noise_id[1] == 1:\n",
    "                    words = self.remove_space(words)\n",
    "                if noise_id[2] == 1:\n",
    "                    words = self.insert_vowel(words)\n",
    "                if noise_id[3] == 1:\n",
    "                    words = self.randomly_lower(words)\n",
    "                if noise_id[4] == 1:\n",
    "                    words = self.remove_apostrophe(words)\n",
    "                if noise_id[5] == 1:\n",
    "                    words = self.remove_consonant(words)\n",
    "                new_corpus.append(' '.join(words))\n",
    "        return new_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "generated\n",
       "0    27371\n",
       "1     5351\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = pd.concat([train_essays,\n",
    "                      Radek_data_gpt_3_5, \n",
    "                     Radek_data_gpt_4, \n",
    "                     PERSUADE_corpus, \n",
    "                     llama_70b_dataset, \n",
    "                     falcon_180b_dataset,\n",
    "                     daigt_external_dataset\n",
    "                    ], \n",
    "                    ignore_index=True)\n",
    "\n",
    "dataset[\"generated\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32722/32722 [00:21<00:00, 1525.79it/s]\n"
     ]
    }
   ],
   "source": [
    "noise = Noise()\n",
    "corpus = noise.add_noise_to_corpus(dataset[\"text\"].tolist())\n",
    "dataset[\"text\"] = corpus\n",
    "# dataset[\"text\"] = [normalize_text(c) for c in corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# very rudimentary cleaning\n",
    "def cleaning(dataset):\n",
    "    \n",
    "    dataset['text'] = dataset['text'].str.strip()\n",
    "    dataset[\"text\"] = dataset[\"text\"].replace('\\\\n',' ')\n",
    "    dataset[\"text\"] = dataset[\"text\"].str.split('ubject: ').str[-1].str.strip()\n",
    "    dataset[\"text\"] = dataset[\"text\"].str.split('Zip').str[-1].str.strip()\n",
    "    dataset[\"text\"] = dataset[\"text\"].str.split('ZIP').str[-1].str.strip()\n",
    "#     dataset = dataset.rename(columns = {'generated':'label'})\n",
    "#     dataset = dataset.drop([\"id\",\"prompt_id\"], axis=1)\n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = cleaning(dataset)\n",
    "dataset.to_csv('train_essays_3.0.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
