{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "PATH = 'data/'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_essays = pd.read_csv(PATH + 'llm-detect-ai-generated-text/train_essays.csv')\n",
    "train_essays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Radek_data_gpt_3_5 = pd.read_csv(PATH + 'llm-generated-essays/ai_generated_train_essays.csv')\n",
    "Radek_data_gpt_4   = pd.read_csv(PATH + 'llm-generated-essays/ai_generated_train_essays_gpt-4.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PERSUADE_corpus = pd.read_csv(PATH + \"persaude-corpus-2/persuade_2.0_human_scores_demo_id_github.csv\")\n",
    "PERSUADE_corpus = PERSUADE_corpus[\"full_text\"].to_frame()\n",
    "PERSUADE_corpus[\"generated\"] = 0 # human data\n",
    "PERSUADE_corpus = PERSUADE_corpus.rename(columns = {'full_text':'text'})\n",
    "PERSUADE_corpus.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llama_70b_dataset = pd.read_csv(\"/kaggle/input/daigt-data-llama-70b-and-falcon180b/llama_70b_v1.csv\")\n",
    "llama_70b_dataset = llama_70b_dataset[\"generated_text\"].to_frame()\n",
    "llama_70b_dataset[\"generated\"] = 1 # AI-LLM data\n",
    "llama_70b_dataset = llama_70b_dataset.rename(columns = {'generated_text':'text'})\n",
    "\n",
    "falcon_180b_dataset = pd.read_csv(\"/kaggle/input/daigt-data-llama-70b-and-falcon180b/falcon_180b_v1.csv\")\n",
    "falcon_180b_dataset = falcon_180b_dataset[\"generated_text\"].to_frame()\n",
    "falcon_180b_dataset[\"generated\"] = 1 # AI-LLM data\n",
    "falcon_180b_dataset = falcon_180b_dataset.rename(columns = {'generated_text':'text'})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "daigt_external_dataset = pd.read_csv(PATH + \"daigt-external-dataset/daigt_external_dataset.csv\")\n",
    "daigt_external_dataset = daigt_external_dataset['text'].to_frame()\n",
    "daigt_external_dataset[\"generated\"] = 1\n",
    "daigt_external_dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import random\n",
    "import numpy as np\n",
    "from tqdm import tqdm \n",
    "\n",
    "\n",
    "BRACKET_SYMBOL = ['[', ']', '(', ')', '{', '}']\n",
    "SPECIAL_CHARACTERS = ['.', '+', '*', '?', '^', '$', '(', ')', '[', ']', '{', '}', '|', '\\\\']\n",
    "CHARACTERS = 'abcdefghijklmnopqrstuvwxyz'\n",
    "VOWEL = 'ueoai'\n",
    "CONSONANTS = 'bcdfghjklmnpqrstvwxz'\n",
    "\n",
    "\n",
    "\n",
    "def normalize_text(text):\n",
    "    text = text.replace('-', '')\n",
    "    for symbol in BRACKET_SYMBOL:\n",
    "        text = text.replace(symbol, f' {symbol} ')\n",
    "\n",
    "    text = re.sub(' +', ' ', text)\n",
    "    return text\n",
    "\n",
    "def is_word(word):\n",
    "    for c in SPECIAL_CHARACTERS:\n",
    "        if c in word:\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "class Noise:    \n",
    "    def remove_consonant(self, words, rate=0.2):\n",
    "        for i, word in enumerate(words):\n",
    "            if is_word(word):\n",
    "                c = random.choice(CONSONANTS)\n",
    "                prob = np.random.uniform(0, 1, 1)\n",
    "                if prob[0] < rate:\n",
    "                    words[i] = words[i].replace(c, '')\n",
    "\n",
    "        return words\n",
    "    \n",
    "    def replace_consonant(self, words):\n",
    "        v = random.choice(VOWEL)\n",
    "        c = random.choice(CONSONANTS)\n",
    "        for i, word in enumerate(words):\n",
    "            if is_word(word):\n",
    "                words[i] = words[i].replace(c, v)\n",
    "                words[i] = words[i].replace(c.upper(), v.upper())\n",
    "        return words\n",
    "    \n",
    "    def remove_space(self, words, n=3):\n",
    "        sentence = ' '.join(words)\n",
    "        space_ids = np.random.randint(0, len(words)-2, n)\n",
    "        phrases = []\n",
    "        for i in space_ids:\n",
    "            phrases.append(f'{words[i]} {words[i+1]}')\n",
    "        for phrase in phrases:\n",
    "            sentence = sentence.replace(phrase, phrase.replace(' ', ''))\n",
    "        return sentence.split()\n",
    "    \n",
    "    def insert_vowel(self, words, rate=0.4):\n",
    "        v = random.choice(VOWEL)\n",
    "        unique_words = list(set(words))\n",
    "        chosen_words = np.random.choice(unique_words, int(rate*len(unique_words)))\n",
    "        inserted_words = []\n",
    "        for word in chosen_words:\n",
    "            id = random.randint(0, len(word)+1)\n",
    "            inserted_words.append(word[:id] + v + word[id:])\n",
    "        sentence = ' '.join(words)\n",
    "        for w, r in zip(chosen_words, inserted_words):\n",
    "            sentence = sentence.replace(w, r)\n",
    "        words = sentence.split()\n",
    "#         print(' '.join(words))\n",
    "#         print('inserted_words: ', inserted_words)\n",
    "        return words\n",
    "    \n",
    "    def randomly_lower(self, words, rate=0.3):\n",
    "        special_words = []\n",
    "        for i, word in enumerate(words):\n",
    "            if i == 0:\n",
    "                continue\n",
    "            if word[0] == word[0].upper() and not is_word(words[i-1]):\n",
    "                special_words.append(word)\n",
    "        n = int(rate*len(special_words)+1)\n",
    "        if n > len(special_words):\n",
    "            return words\n",
    "        chosen_words = np.random.choice(special_words, n)\n",
    "        \n",
    "        sentence = ' '.join(words)\n",
    "        for word in chosen_words:\n",
    "            sentence = sentence.replace(word, word.lower())\n",
    "        \n",
    "        return sentence.split()\n",
    "    \n",
    "    def remove_apostrophe(self, words, rate=0.2):\n",
    "        sentence = ' '.join(words)\n",
    "        apost_phrase = re.findall('[^ ]*\\'[^ ]*', sentence)\n",
    "        if int(rate*len(apost_phrase)+1) > len(apost_phrase):\n",
    "            return words\n",
    "        chosen_words = np.random.choice(apost_phrase, int(rate*len(apost_phrase)+1))\n",
    "        for word in chosen_words:\n",
    "            sentence = sentence.replace(word, word.replace(\"'\", \"\"))\n",
    "        return sentence.split()\n",
    "    \n",
    "    def add_noise_to_corpus(self, corpus, rate=0.9):\n",
    "        prob = np.random.uniform(0, 1, len(corpus))\n",
    "        choice = prob > rate\n",
    "        new_corpus = []\n",
    "        for i in tqdm(range(len(corpus))):\n",
    "            if choice[i]:\n",
    "                new_corpus.append(corpus[i])\n",
    "            else:\n",
    "                words = corpus[i].split()\n",
    "                noise_id = np.random.randint(0, 2, 6)\n",
    "                if noise_id[0] == 1:\n",
    "                    words = self.replace_consonant(words)\n",
    "                if noise_id[1] == 1:\n",
    "                    words = self.remove_space(words)\n",
    "                if noise_id[2] == 1:\n",
    "                    words = self.insert_vowel(words)\n",
    "                if noise_id[3] == 1:\n",
    "                    words = self.randomly_lower(words)\n",
    "                if noise_id[4] == 1:\n",
    "                    words = self.remove_apostrophe(words)\n",
    "                if noise_id[5] == 1:\n",
    "                    words = self.remove_consonant(words)\n",
    "                new_corpus.append(' '.join(words))\n",
    "        return new_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.concat([train_essays,\n",
    "                      Radek_data_gpt_3_5, \n",
    "                     Radek_data_gpt_4, \n",
    "                     PERSUADE_corpus, \n",
    "                     llama_70b_dataset, \n",
    "                     falcon_180b_dataset,\n",
    "                     daigt_external_dataset\n",
    "                    ], \n",
    "                    ignore_index=True)\n",
    "\n",
    "dataset[\"generated\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise = Noise()\n",
    "corpus = noise.add_noise_to_corpus(dataset[\"text\"].tolist())\n",
    "dataset[\"text\"] = corpus\n",
    "# dataset[\"text\"] = [normalize_text(c) for c in corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# very rudimentary cleaning\n",
    "def cleaning(dataset):\n",
    "    \n",
    "    dataset['text'] = dataset['text'].str.strip()\n",
    "    dataset[\"text\"] = dataset[\"text\"].replace('\\\\n',' ')\n",
    "    dataset[\"text\"] = dataset[\"text\"].str.split('ubject: ').str[-1].str.strip()\n",
    "    dataset[\"text\"] = dataset[\"text\"].str.split('Zip').str[-1].str.strip()\n",
    "    dataset[\"text\"] = dataset[\"text\"].str.split('ZIP').str[-1].str.strip()\n",
    "#     dataset = dataset.rename(columns = {'generated':'label'})\n",
    "#     dataset = dataset.drop([\"id\",\"prompt_id\"], axis=1)\n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = cleaning(dataset)\n",
    "dataset.to_csv('train_essays_3.0.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
